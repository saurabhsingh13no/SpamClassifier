{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The approach in this notebook is same as Andrew Ng's Sequence Modelling class (Deep Learning Specialization) on [coursera](https://www.coursera.org/learn/nlp-sequence-models) where he taught us to emojify a sentence. \n",
    "\n",
    "I am using this approach in slight different way to classify a sentence as spam or not-spam.\n",
    "\n",
    "Credit for most of the code goes to [Andrew Ng](http://www.andrewng.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processedFilename = \"../data/processedFile.csv\"\n",
    "gloveVecFile = \"../data/glove.6B.50d.txt\"\n",
    "maxLen = 20 # Computed in the previous notebook : 2_ExploratoryDataAnalysis.ipynb\n",
    "classes = 2 # equals to number of classes of spam. Here we have 2. Spam and Not-Spam\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "TrainTestPartition = 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set : 4121, and testing set : 1018\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah i don t think he goes to usf he lives arou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>as per your request melle melle oru minnaminun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "1                            ok lar joking wif u oni  0\n",
       "3        u dun say so early hor u c already then say  0\n",
       "4  nah i don t think he goes to usf he lives arou...  0\n",
       "6  even my brother is not like to speak with me t...  0\n",
       "7  as per your request melle melle oru minnaminun...  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def partition(filename, TrainTestPartition):\n",
    "    \"\"\"\n",
    "    Function that partitions the data in 'filename' into training and testing based on the ratio in \n",
    "    TrainTestPartition\n",
    "    :param filname : String. e.g. \"../data/processedFile.csv\"\n",
    "    :param TrainTestPartition : Integer . e.g. 0.80\n",
    "    :return : Tuple(DataFrame, DataFrame)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    msk = np.random.rand(len(df)) < TrainTestPartition\n",
    "    \n",
    "    return df[msk], df[~msk]\n",
    "\n",
    "trainDf, testDf = partition(processedFilename, TrainTestPartition)\n",
    "print (\"Length of training set : {0}, and testing set : {1}\".format(len(trainDf), len(testDf)))\n",
    "trainDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building X_train, Y_train, X_test and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ok lar joking wif u oni',\n",
       "       'u dun say so early hor u c already then say',\n",
       "       'nah i don t think he goes to usf he lives around here though',\n",
       "       'even my brother is not like to speak with me they treat me like aids patent',\n",
       "       'as per your request melle melle oru minnaminunginte nurungu vettam has been set as your callertune for all callers press 9 to copy your friends callertune'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train = trainDf[0].values, trainDf[1].values\n",
    "X_test, Y_test = testDf[0].values, testDf[1].values\n",
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Y to one-hot\n",
    "\n",
    "Getting data ready for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_one_hot(Y, classes=2):\n",
    "    \"\"\"\n",
    "    Function to convert Y into one-hot depending on the classes\n",
    "    :param Y : numpy_array(Integer)\n",
    "    :param classes : Integer. e.g 2\n",
    "    :return : numpy_array(List(Integers))\n",
    "    \"\"\"\n",
    "    Y = np.eye(classes)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "Y_OneHot_Train = convert_to_one_hot(Y_train, classes)\n",
    "Y_OneHot_Test = convert_to_one_hot(Y_test, classes)\n",
    "Y_OneHot_Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Glove Vector\n",
    "\n",
    "Glove vector would help us convert each word into array of Integers. These glove vectors has already been [downloaded](https://nlp.stanford.edu/projects/glove/) into `../data/` folder.\n",
    "\n",
    "If you do not have this file. Download these from the above link. I will be using 50-Dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGlove(filename):\n",
    "    \"\"\"\n",
    "    Function to read glove vector from the disk and compute word_to_index, index_to_word, and word_to_vec map.    \n",
    "    Below code is taken from Deep Leaning class of Andrew Ng on coursera.\n",
    "    \n",
    "    :param filename : String. e.g. \"../data/gloveVec.txt\"\n",
    "    :return : Tuple(dict, dict, dict) .\n",
    "    \"\"\"        \n",
    "    with open(filename, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "words_to_index, index_to_words, word_to_vec_map = readGlove(gloveVecFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.68095 , -1.5925  , -0.28595 , -0.148   , -0.85375 , -0.3206  ,\n",
       "        0.56763 , -0.55571 ,  0.029921, -0.19592 , -0.43244 ,  0.077961,\n",
       "        0.3094  , -0.51442 , -1.3036  , -0.54708 ,  0.81773 , -0.24174 ,\n",
       "       -0.018879,  0.46546 , -0.79265 , -0.28118 , -0.25071 , -0.60221 ,\n",
       "        0.53435 ,  0.60365 , -1.435   , -1.0093  , -0.63034 , -0.50105 ,\n",
       "       -0.66707 , -0.13999 ,  0.23964 ,  1.0034  ,  0.5395  ,  0.33595 ,\n",
       "        0.59106 , -0.48929 , -0.02608 ,  0.44607 , -0.2265  ,  0.45986 ,\n",
       "       -0.45285 , -0.59165 ,  0.23867 , -0.46094 ,  0.46202 , -0.3993  ,\n",
       "        0.52779 ,  0.9966  ])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec_map['lar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting sentences from X_train/X_test into indices vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()`\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. One can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros(shape=(m, max_len))\n",
    "    \n",
    "    for i in range(m):                               \n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words =X[i].lower().strip().split()\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            try:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            except KeyError as e:\n",
    "                X_indices[i, j] = word_to_index[\"unk\"]\n",
    "            # Increment j to j + 1\n",
    "            j = j + 1\n",
    "            if j>=max_len:\n",
    "                break\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining pre-training embedding layer to be used in out LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      \n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros(shape=(vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable.\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer.\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spamify(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the spamify model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = LSTM(128, return_sequences = True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(rate = 0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(classes, activation='softmax')(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(sentence_indices, X)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 20, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 20, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,540.0\n",
      "Trainable params: 223,490.0\n",
      "Non-trainable params: 20,000,050.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = spamify((maxLen,), word_to_vec_map, words_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model \n",
    "\n",
    "As usual, after creating your model in Keras, we need to compile it and define what loss, optimizer and metrics we want to use. Compiling my model using categorical_crossentropy loss, adam optimizer and ['accuracy'] metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train our model. Our spamify `model` takes as input an array of shape (`m`, `max_len`) and outputs probability vectors of shape (`m`, `number of classes`). We thus have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices), and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, words_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4121/4121 [==============================] - 15s - loss: 0.4390 - acc: 0.8743    \n",
      "Epoch 2/50\n",
      "4121/4121 [==============================] - 15s - loss: 0.4390 - acc: 0.8743    \n",
      "Epoch 3/50\n",
      "4121/4121 [==============================] - 14s - loss: 0.4390 - acc: 0.8743    \n",
      "Epoch 4/50\n",
      "3250/4121 [======================>.......] - ETA: 3s - loss: 0.4456 - acc: 0.8677"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 268745.,  217389.,  198528., ...,       0.,       0.,       0.],\n",
       "       [ 369052.,  130722.,  319691., ...,       0.,       0.,       0.],\n",
       "       [ 255601.,  185457.,  127406., ...,       0.,       0.,       0.],\n",
       "       ..., \n",
       "       [ 285736.,  383514.,  188481., ...,       0.,       0.,       0.],\n",
       "       [ 357266.,  169725.,  123517., ...,  260309.,  384714.,   54718.],\n",
       "       [ 372306.,  193919.,  366081., ...,       0.,       0.,       0.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
