{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The approach in this notebook is same as Andrew Ng's Sequence Modelling class (Deep Learning Specialization) on [coursera](https://www.coursera.org/learn/nlp-sequence-models) where he taught us to emojify a sentence. \n",
    "\n",
    "I am using this approach in slight different way to classify a sentence as spam or not-spam.\n",
    "\n",
    "Credit for most of the code goes to [Andrew Ng](http://www.andrewng.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processedFilename = \"../data/processedFile.csv\"\n",
    "uniformProcessedFilename = \"../data/uniformDataProcessedFile.csv\"\n",
    "gloveVecFile = \"../data/glove.6B.50d.txt\"\n",
    "# gloveVecFile = \"../data/glove.twitter.27B.50d.txt\"   # New twitter glove file\n",
    "maxLen = 30 # Computed in the previous notebook : 2_ExploratoryDataAnalysis.ipynb\n",
    "classes = 2 # equals to number of classes of spam. Here we have 2. Spam and Not-Spam\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "TrainTestPartition = 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set : 1024, and testing set : 269\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi its lucy hubby at meetins all day fri i wil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>think ur smart win 200 this week in our weekly...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>guess what somebody you know secretly fancies ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sorry for the delay yes masters</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you ve won tkts to the euro2004 cup final or 8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  hi its lucy hubby at meetins all day fri i wil...  1\n",
       "1  think ur smart win 200 this week in our weekly...  1\n",
       "2  guess what somebody you know secretly fancies ...  1\n",
       "3                    sorry for the delay yes masters  0\n",
       "4  you ve won tkts to the euro2004 cup final or 8...  1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def partition(filename, TrainTestPartition):\n",
    "    \"\"\"\n",
    "    Function that partitions the data in 'filename' into training and testing based on the ratio in \n",
    "    TrainTestPartition\n",
    "    :param filname : String. e.g. \"../data/processedFile.csv\"\n",
    "    :param TrainTestPartition : Integer . e.g. 0.80\n",
    "    :return : Tuple(DataFrame, DataFrame)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    msk = np.random.rand(len(df)) < TrainTestPartition\n",
    "    \n",
    "    return df[msk], df[~msk]\n",
    "\n",
    "trainDf, testDf = partition(uniformProcessedFilename, TrainTestPartition)\n",
    "print (\"Length of training set : {0}, and testing set : {1}\".format(len(trainDf), len(testDf)))\n",
    "trainDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 523, 1: 501})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(trainDf[1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building X_train, Y_train, X_test and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'hi its lucy hubby at meetins all day fri i will b alone at hotel u fancy cumin over pls leave msg 2day 09099726395 lucy x calls 1 minmobsmorelkpobox177hp51fl',\n",
       "       'think ur smart win 200 this week in our weekly quiz text play to 85222 now t cs winnersclub po box 84 m26 3uz 16 gbp1 50 week',\n",
       "       'guess what somebody you know secretly fancies you wanna find out who it is give us a call on 09065394514 from landline datebox1282essexcm61xn 150p min 18',\n",
       "       'sorry for the delay yes masters',\n",
       "       'you ve won tkts to the euro2004 cup final or 800 cash to collect call 09058099801 b4190604 pobox 7876150ppm'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train = trainDf[0].values, trainDf[1].values\n",
    "X_test, Y_test = testDf[0].values, testDf[1].values\n",
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Y to one-hot\n",
    "\n",
    "Getting data ready for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_one_hot(Y, classes=2):\n",
    "    \"\"\"\n",
    "    Function to convert Y into one-hot depending on the classes\n",
    "    :param Y : numpy_array(Integer)\n",
    "    :param classes : Integer. e.g 2\n",
    "    :return : numpy_array(List(Integers))\n",
    "    \"\"\"\n",
    "    Y = np.eye(classes)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "Y_OneHot_Train = convert_to_one_hot(Y_train, classes)\n",
    "Y_OneHot_Test = convert_to_one_hot(Y_test, classes)\n",
    "Y_OneHot_Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Glove Vector\n",
    "\n",
    "Glove vector would help us convert each word into array of Integers. These glove vectors has already been [downloaded](https://nlp.stanford.edu/projects/glove/) into `../data/` folder.\n",
    "\n",
    "If you do not have this file. Download these from the above link. I will be using 50-Dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGlove(filename):\n",
    "    \"\"\"\n",
    "    Function to read glove vector from the disk and compute word_to_index, index_to_word, and word_to_vec map.    \n",
    "    Below code is taken from Deep Leaning class of Andrew Ng on coursera.\n",
    "    \n",
    "    :param filename : String. e.g. \"../data/gloveVec.txt\"\n",
    "    :return : Tuple(dict, dict, dict) .\n",
    "    \"\"\"        \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "words_to_index, index_to_words, word_to_vec_map = readGlove(gloveVecFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.68224 , -0.31608 , -0.95201 ,  0.47108 ,  0.56571 ,  0.13151 ,\n",
       "        0.22457 ,  0.094995, -1.3237  , -0.51545 , -0.39337 ,  0.88488 ,\n",
       "        0.93826 ,  0.22931 ,  0.088624, -0.53908 ,  0.23396 ,  0.73245 ,\n",
       "       -0.019123, -0.26552 , -0.40433 , -1.5832  ,  1.1316  ,  0.4419  ,\n",
       "       -0.48218 ,  0.4828  ,  0.14938 ,  1.1245  ,  1.0159  , -0.50213 ,\n",
       "        0.83831 , -0.31303 ,  0.083242,  1.7161  ,  0.15024 ,  1.0324  ,\n",
       "       -1.5005  ,  0.62348 ,  0.54508 , -0.88484 ,  0.53279 , -0.085119,\n",
       "        0.02141 , -0.56629 ,  1.1463  ,  0.6464  ,  0.78318 , -0.067662,\n",
       "        0.22884 , -0.042453])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec_map['cucumber']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting sentences from X_train/X_test into indices vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()`\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. One can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape\n",
    "    X_indices = np.zeros(shape=(m, max_len))\n",
    "    \n",
    "    for i in range(m):                               \n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words =X[i].lower().strip().split()\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            try:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            except KeyError as e:\n",
    "                X_indices[i, j] = word_to_index[\"unk\"]\n",
    "            # Increment j to j + 1\n",
    "            j = j + 1\n",
    "            if j>=max_len:\n",
    "                break\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining pre-training embedding layer to be used in out LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "#     vocab_len = len(words_to_index)                     #using this for twitter glove vectors\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      \n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros(shape=(vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        try:\n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        except Exception as e:\n",
    "            print (\"Exception {0} occured for word {1}\".format(e, word))\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable.\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer.\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spamify(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the spamify model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = LSTM(128, return_sequences = True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(rate = 0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(classes, activation='softmax')(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(sentence_indices, X)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 30, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,540.0\n",
      "Trainable params: 223,490.0\n",
      "Non-trainable params: 20,000,050.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = spamify((maxLen,), word_to_vec_map, words_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model \n",
    "\n",
    "As usual, after creating your model in Keras, we need to compile it and define what loss, optimizer and metrics we want to use. Compiling my model using categorical_crossentropy loss, adam optimizer and ['accuracy'] metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train our model. Our spamify `model` takes as input an array of shape (`m`, `max_len`) and outputs probability vectors of shape (`m`, `number of classes`). We thus have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices), and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, words_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.5194 - acc: 0.8242     \n",
      "Epoch 2/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.4031 - acc: 0.9043     \n",
      "Epoch 3/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3793 - acc: 0.9346     \n",
      "Epoch 4/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3691 - acc: 0.9443     \n",
      "Epoch 5/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3605 - acc: 0.9541     \n",
      "Epoch 6/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3522 - acc: 0.9609     \n",
      "Epoch 7/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3540 - acc: 0.9561     \n",
      "Epoch 8/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3474 - acc: 0.9648     \n",
      "Epoch 9/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3526 - acc: 0.9590     \n",
      "Epoch 10/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3456 - acc: 0.9688     \n",
      "Epoch 11/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3576 - acc: 0.9482     \n",
      "Epoch 12/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3564 - acc: 0.9551     \n",
      "Epoch 13/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3495 - acc: 0.9629     \n",
      "Epoch 14/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3449 - acc: 0.9687     \n",
      "Epoch 15/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3417 - acc: 0.9717     \n",
      "Epoch 16/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3426 - acc: 0.9707     \n",
      "Epoch 17/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3399 - acc: 0.9727     \n",
      "Epoch 18/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3370 - acc: 0.9775     \n",
      "Epoch 19/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3354 - acc: 0.9775     \n",
      "Epoch 20/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3431 - acc: 0.9727     \n",
      "Epoch 21/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3365 - acc: 0.9766     \n",
      "Epoch 22/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3348 - acc: 0.9785     \n",
      "Epoch 23/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3342 - acc: 0.9795     \n",
      "Epoch 24/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3403 - acc: 0.9717     \n",
      "Epoch 25/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3420 - acc: 0.9727     \n",
      "Epoch 26/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3453 - acc: 0.9687     \n",
      "Epoch 27/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3383 - acc: 0.9746     \n",
      "Epoch 28/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3376 - acc: 0.9746     \n",
      "Epoch 29/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3396 - acc: 0.9727     \n",
      "Epoch 30/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3353 - acc: 0.9785     \n",
      "Epoch 31/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3321 - acc: 0.9814     \n",
      "Epoch 32/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3320 - acc: 0.9814     \n",
      "Epoch 33/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3311 - acc: 0.9824     \n",
      "Epoch 34/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3469 - acc: 0.9648     \n",
      "Epoch 35/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3422 - acc: 0.9697     \n",
      "Epoch 36/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3843 - acc: 0.9258     \n",
      "Epoch 37/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3468 - acc: 0.9648     \n",
      "Epoch 38/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3403 - acc: 0.9736     \n",
      "Epoch 39/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3534 - acc: 0.9600     \n",
      "Epoch 40/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3751 - acc: 0.9375     \n",
      "Epoch 41/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3451 - acc: 0.9668     \n",
      "Epoch 42/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3351 - acc: 0.9775     \n",
      "Epoch 43/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3370 - acc: 0.9766     \n",
      "Epoch 44/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3325 - acc: 0.9795     \n",
      "Epoch 45/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3300 - acc: 0.9834     \n",
      "Epoch 46/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3305 - acc: 0.9824     \n",
      "Epoch 47/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3341 - acc: 0.9785     \n",
      "Epoch 48/50\n",
      "1024/1024 [==============================] - 5s - loss: 0.3326 - acc: 0.9795     \n",
      "Epoch 49/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3307 - acc: 0.9824     \n",
      "Epoch 50/50\n",
      "1024/1024 [==============================] - 4s - loss: 0.3324 - acc: 0.9824     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f975982a048>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 50, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/269 [===========================>..] - ETA: 0s\n",
      "Test accuracy =  0.944237918216\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, words_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, classes = classes)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/269 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predValues (X_test_indices):\n",
    "    \"\"\"\n",
    "    Function to predict whether text is spam or not\n",
    "    :param X_test_indices : np.array(list(integers))\n",
    "    :return : np.array(Integers)\n",
    "    \"\"\"\n",
    "    pred = model.predict(X_test_indices, verbose=1)\n",
    "    pred_array_values = []\n",
    "    for prediction in pred:\n",
    "        pred_array_values.append(np.argmax(prediction))\n",
    "        \n",
    "    return pred_array_values\n",
    "    \n",
    "pred_array_values = predValues(X_test_indices)    \n",
    "pred_array_values[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the confusion matrix : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[125,   6],\n",
       "       [  9, 129]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Below is the confusion matrix : \")\n",
    "confusion_matrix(Y_test, pred_array_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Non-Spam       0.93      0.95      0.94       131\n",
      "       Spam       0.96      0.93      0.95       138\n",
      "\n",
      "avg / total       0.94      0.94      0.94       269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Non-Spam', 'Spam']\n",
    "print (classification_report(Y_test, pred_array_values, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on random sentence\n",
    "\n",
    "Use the below code to experiment with you own sentence. Type your sentence and see whether the model is able to classify it corretly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_sms preprocessed : \tt mobile customer you may now claim your free camera phone upgrade a pay go sim card for your loyalty call on 0845 021 3680 offer ends 28thfeb t c s apply\n",
      "\n",
      "1/1 [==============================] - 0s\n",
      "\n",
      "Given text is : SPAM\n"
     ]
    }
   ],
   "source": [
    "user_sms = \"t mobile customer you may now claim your... free camera phone upgrade a pay go sim card for your loyalty call on 0845 021 3680 offer ends 28thfeb t c s apply\"\n",
    "# user_sms = \"Hi Saurabh. How are you\"\n",
    "# user_sms = \"Hey saurabh, ..../././././ get our free bill\"\n",
    "user_sms = user_sms.strip().lower()\n",
    "pattern = re.compile('[^A-Za-z0-9]+')\n",
    "user_sms = re.sub(pattern, \" \", user_sms)\n",
    "print (\"user_sms preprocessed : \\t{0}\\n\".format(user_sms))\n",
    "\n",
    "user_sms_indices = sentences_to_indices(np.array([user_sms]), words_to_index, maxLen) \n",
    "pred_values = predValues(user_sms_indices)\n",
    "print (\"\\nGiven text is : {0}\".format(\"SPAM\" if pred_values[0]==1 else \"Not a Spam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
